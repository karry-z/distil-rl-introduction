
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 6. Temporal-Difference Learning &#8212; DistilRLIntro 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=e1a75a79"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Contents/6_temporal_difference_learning';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 7. Planning and Learning with Tabular Methods" href="7_planning_learning_acting.html" />
    <link rel="prev" title="Chapter 5. Monte Carlo Methods" href="5_monte_carlo_methods.html" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />   
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">⚠️This is an ongoing project and is currectly still under development (Chinese version on its way). ⚠️</div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="DistilRLIntro 0.1 documentation - Home"/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="DistilRLIntro 0.1 documentation - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">I. Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_prelude.html">Chapter 0. Prelude</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_intro.html">Chapter 1. Introduction to RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II.1. Fundamentals of Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_multi_armed_bandits.html">Chapte 2. Multi-armed Bandit</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_markov_decision_process.html">Chapter 3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_dynamic_programming.html">Chapter 4. Dynamic Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II.2. Sample-based Learning Methods</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="5_monte_carlo_methods.html">Chapter 5. Monte Carlo Methods</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 6. Temporal-Difference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_planning_learning_acting.html">Chapter 7. Planning and Learning with Tabular Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III.1. Value Function Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="8_on_policy_prediction_with_approximation.html">Chapter 8. On-policy Prediction with Approximation</a></li>
<li class="toctree-l1"><a class="reference internal" href="9_on_policy_control_with_approximation.html">Chapter 9. On-policy Control with Approximation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III.2. Policy Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_policy_gradient_methods.html">Chapter 10. Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_modern_policy_gradient_methods.html">Chapter 11. Modern Policy Gradient Methods</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/blob/master/Contents/6_temporal_difference_learning.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/edit/master/Contents/6_temporal_difference_learning.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/issues/new?title=Issue%20on%20page%20%2FContents/6_temporal_difference_learning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Contents/6_temporal_difference_learning.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 6. Temporal-Difference Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#td-prediction">6.1 TD prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#td-control">6.2 TD Control</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sarsa-on-policy-td-control">6.2.1 Sarsa: On-policy TD Control</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning-off-policy-td-control">6.2.2 Q-learning: Off-policy TD Control</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-sarsa">6.2.3 Expected Sarsa</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">6.3 Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                   <section class="tex2jax_ignore mathjax_ignore" id="chapter-6-temporal-difference-learning">
<h1>Chapter 6. Temporal-Difference Learning<a class="headerlink" href="#chapter-6-temporal-difference-learning" title="Link to this heading">#</a></h1>
<p>Temporal-Difference (TD) Learning is a combination of Monte Carlo ideas and Dynamic Programming (DP). Like MC methods, TD methods <strong>can learn directly from raw experience</strong> without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (<strong>they bootstrap</strong>).</p>
<p>As usual, we start by focusing on the policy evaluation or prediction problem, the problem of estimating the value function <span class="math notranslate nohighlight">\(v_\pi\)</span> for a given policy <span class="math notranslate nohighlight">\(\pi\)</span>. For the control problem (finding an optimal policy), DP, TD, and MC methods all use some variation of generalized policy iteration (GPI). The differences in the methods are primarily differences in their approaches to the prediction problem.</p>
<section id="td-prediction">
<h2>6.1 TD prediction<a class="headerlink" href="#td-prediction" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>To introduce TD methods, first recall that</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
		\begin{align}
		v_{\pi}(s) &amp;\dot= E[G_t | S_t = s] \\
		&amp;= E[R_{t+1} + \gamma G_{t+1}| S_t = s] \\
		&amp;= E[R_{t+1} + \gamma v_{\pi}(S_{t+1})| S_t = s]
		\end{align}
	\end{split}\]</div>
<ul>
<li><p><strong>Monte Carlo methods</strong> use an estimate of equation (1) as a target:</p>
<p><span class="math notranslate nohighlight">\(V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))\)</span> with <span class="math notranslate nohighlight">\(G_t\)</span> as the single realisation from each episode .</p>
</li>
<li><p><strong>Dynamic Programming methods</strong> use an estiamte of equation (3) as a target:</p>
<p><span class="math notranslate nohighlight">\(V(s) = \sum_a \pi(a|s) \sum_{s', r} p(s',r|s,a) [r + \gamma V(s')]\)</span></p>
</li>
<li><p><strong>Temporal Difference methods</strong> also uses an estimate of equation (3), yet requires no model of the environment’s dynamics. It combines the above two in the following way:</p>
<p><span class="math notranslate nohighlight">\(V(S_t) \leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))\)</span> with <span class="math notranslate nohighlight">\(R_{t+1} + \gamma V(S_{t+1})\)</span> as the single realisation of <span class="math notranslate nohighlight">\(R_{t+1} + \gamma v_{\pi}(S_{t+1})\)</span> each time.</p>
<ul class="simple">
<li><p>The target for the TD update is <span class="math notranslate nohighlight">\(R_{t+1} + \gamma V(S_{t+1})\)</span>, this TD method is called <strong>TD(0) or one-step TD</strong>, which is a special case of the TD(<span class="math notranslate nohighlight">\(\lambda\)</span>) or n-step TD (not included in this tutorial)</p></li>
<li><p>TD combines the sampling of MC and the bootstrapping of DP:</p>
<ol class="arabic simple">
<li><p>TD and MC both involve looking ahead to a sample successor state (or state–action pair), i.e., they both use <span class="math notranslate nohighlight">\(\textit{sample update}\)</span> instead of <span class="math notranslate nohighlight">\(\textit{expected update}\)</span> as in DP.</p></li>
<li><p>Whereas MC methods must wait until the end of the episode to determine the increment to <span class="math notranslate nohighlight">\(V(S_t)\)</span> (only then is <span class="math notranslate nohighlight">\(G_t\)</span> known), TD and DP methods need to wait only until the next time step, i.e., they bootstrap.</p></li>
</ol>
</li>
<li><p>The term <span class="math notranslate nohighlight">\(\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\)</span> is called <span class="math notranslate nohighlight">\(\textit{TD error}\)</span>. <span class="math notranslate nohighlight">\(\delta_t\)</span> is the error in <span class="math notranslate nohighlight">\(V(S_t)\)</span>, yet available at time <span class="math notranslate nohighlight">\(t+1\)</span>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Tabular TD(0) for estimating <span class="math notranslate nohighlight">\(v_\pi\)</span></strong>:</p>
<ul>
<li><p>Algorithm:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter6/algo_tabular_td.png" alt="Algorithm: Tabular TD(0)" style="width: 100%;;">
  </div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Notice the difference between MC algorithms, there is now no need to generate the whole episode. Instead, <span class="math notranslate nohighlight">\(V(s)\)</span> will be updated right after an action is taken and a new state is observed, i.e., TD methods work in a “step-wise” fashion.</p>
</div>
</li>
</ul>
</li>
<li><p><strong>Example of Driving Home</strong> (Click to watch the lecture video)</p>
  <a href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/9Dxlq/the-advantages-of-temporal-difference-learning">
  <img src="../_static/img/chapter6/driving_home.png" alt="Video: TD prediction on Driving Home Example" style="width:70%;">
  </a>
</li>
<li><p><strong>Advantages of TD prediction</strong>:</p>
<ul>
<li><p>Over dynamic programming: TD methods do not need a model of the environment. Over Monte Carlo: TD methods are naturally implemented in an online, fully incremental fashion, i.e., they do not require to wait until the end of an episode.</p></li>
<li><p>For any fixed policy <span class="math notranslate nohighlight">\(\pi\)</span>, TD(0) has been proved to converge to <span class="math notranslate nohighlight">\(v_\pi\)</span>. For details, refer to the book chapter 6.2, we skip the proof in this tutorial.</p></li>
<li><p>In practice, TD methods have usually been found to  converge faster than constant-<span class="math notranslate nohighlight">\(\alpha\)</span> MC methods on stochastic tasks. A demonstrative example is given in the following video:</p>
  <a href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/CEzFc/comparing-td-and-monte-carlo">
  <img src="../_static/img/chapter6/random_walk.png" alt="Video: Comparing TD and MC" style="width:70%;">
  </a>			
</li>
</ul>
</li>
</ul>
</section>
<section id="td-control">
<h2>6.2 TD Control<a class="headerlink" href="#td-control" title="Link to this heading">#</a></h2>
<section id="sarsa-on-policy-td-control">
<h3>6.2.1 Sarsa: On-policy TD Control<a class="headerlink" href="#sarsa-on-policy-td-control" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Backgound for Sarsa</strong>:</p>
<ul>
<li><p>Since TD methods deal with tasks where there is no model of environment available, it is natural to estimate <span class="math notranslate nohighlight">\(Q_\pi(s,a)\)</span> instead of <span class="math notranslate nohighlight">\(V_\pi(s)\)</span>. Similar to <a class="reference internal" href="#td-prediction">section 6.1</a>, the <strong>update rule for Sarsa</strong> is:</p>
<div class="math notranslate nohighlight">
\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\]</div>
<p>with <span class="math notranslate nohighlight">\(Q(S_t, A_t)=0\)</span> if <span class="math notranslate nohighlight">\(S_t\)</span> is the terminal state.</p>
</li>
<li><p><strong>Naming of Sarsa</strong>: the above update rule uses every element of the quintuple of events, (<span class="math notranslate nohighlight">\(S_t,A_t,R_{t+1},S_{t+1},A_{t+1}\)</span>), that make up a transition from one state–action pair to the next. This quintuple gives rise to the name Sarsa (<strong>S</strong>tate, <strong>A</strong>ction, <strong>R</strong>eward, <strong>S</strong>tate, <strong>A</strong>ction)</p></li>
<li><p>Similar to any other on-policy methods, we continually estimate <span class="math notranslate nohighlight">\(q_\pi\)</span> for the behavior policy <span class="math notranslate nohighlight">\(\pi\)</span>, and at the same time change <span class="math notranslate nohighlight">\(\pi\)</span> toward greediness with respect to <span class="math notranslate nohighlight">\(q_\pi\)</span> (pattern of generalized policy iteration (GPI)).</p></li>
</ul>
</li>
<li><p><strong>Sarsa (on-policy TD control) for estimating <span class="math notranslate nohighlight">\(Q \approx q_{\star}\)</span></strong></p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter6/algo_sarsa.png" alt="Algorithm: SARSA" style="width: 100%;;">
  </div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><strong>There is no need to initialize a policy <span class="math notranslate nohighlight">\(\pi\)</span> in the beginning</strong>, the action can be derived directly from a given policy wth <span class="math notranslate nohighlight">\(Q(s,a), \text{ for all } s \in S, a \in A(s)\)</span> available.</p></li>
<li><p>While deriving the next action, make sure to use a soft-policy to ensure exploration.</p></li>
<li><p>Notice that after transit to the state <span class="math notranslate nohighlight">\(S'\)</span>, you still need to take another action <span class="math notranslate nohighlight">\(A'\)</span> to be able to update <span class="math notranslate nohighlight">\(Q(S,A)\)</span>.</p></li>
<li><p>Sarsa converges with probability 1 to an optimal policy and action-value function as long as all state–action pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy (which can be arranged, for example, with <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policies by setting <span class="math notranslate nohighlight">\(\epsilon = 1/t\)</span>).</p></li>
</ul>
</div>
</li>
<li><p><strong>Example: Sarsa in the Windy Gridworld</strong></p>
  <a href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/RZeRQ/sarsa-in-the-windy-grid-world">
  <img src="../_static/img/chapter6/windy_gridworld.png" alt="Video: Sarsa in the Windy Gridworld" style="width:70%;">
  </a>
<ul class="simple">
<li><p>Notice that the first few episodes take a couple thousand steps to complete. The curve gradually gets steeper indicating that episodes are completed more quickly.</p></li>
<li><p>Notice the episode completion rate stops increasing. This means the agents policy hovers around the optimal policy and won’t be exactly optimal, because of exploration.</p></li>
</ul>
</li>
</ul>
</section>
<section id="q-learning-off-policy-td-control">
<h3>6.2.2 Q-learning: Off-policy TD Control<a class="headerlink" href="#q-learning-off-policy-td-control" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Backgound for Q-learning</strong>:</p>
<ul>
<li><p>The update rule for Q-learning is:</p>
<div class="math notranslate nohighlight">
\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\]</div>
</li>
<li><p>This way of directly approximating <span class="math notranslate nohighlight">\(q_\star\)</span> dramatically simplifies the analysis of the algorithm and enabled early convergence proofs. All that is required for correct convergence is that all pairs continue to be updated (exploration).</p></li>
</ul>
</li>
<li><p><strong><span class="math notranslate nohighlight">\(Q\)</span>-learning (off-policy TD control) for estimating <span class="math notranslate nohighlight">\(\pi \approx \pi_{\star}\)</span></strong></p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter6/algo_q_learning.png" alt="Algorithm: Q-Learning" style="width: 100%;;">
  </div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><strong>Different from Sarsa, at target state <span class="math notranslate nohighlight">\(S'\)</span>, Q-learning choose the greedy action that maximizes <span class="math notranslate nohighlight">\(Q(S', a)\)</span> directly</strong>, but not according to a policy derived from <span class="math notranslate nohighlight">\(Q\)</span> (although, the derived policy from <span class="math notranslate nohighlight">\(Q\)</span> can also be the greedy policy, if so, the update rules of Sarsa and Q-learning are identical).</p></li>
<li><p><strong>Q-learning is off-policy, but why?</strong> Consider the derived policy from current <span class="math notranslate nohighlight">\(Q\)</span> as the <span class="math notranslate nohighlight">\(\textit{behaviour policy}\)</span>, which can be e.g., <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy. but the <span class="math notranslate nohighlight">\(\textit{target policy}\)</span> for Q-learning is actually the greedy policy according to the <span class="math notranslate nohighlight">\(max\)</span> term in the update rule from above (actions are chosen according to <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy, updates are made according to the greedy policy). Readers of interest about why exactly Q-learning is off-policy can further refer to this <a class="reference external" href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/1OikH/how-is-q-learning-off-policy">lecture video.</a></p></li>
</ul>
</div>
</li>
<li><p><strong>Example: Q-learning in the Windy Gridworld</strong></p>
  <a href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/BZbSy/q-learning-in-the-windy-grid-world">
  <img src="../_static/img/chapter6/q_learning_windy_gridworld.png" alt="Video: Sarsa in the Windy Gridworld" style="width:70%;">
  </a>
<ul class="simple">
<li><p>In the beginning, the two algorithms learn at a similar pace. Towards the end, Q-Learning seems to learn a better final policy.</p></li>
<li><p>When we descrease the step-size <span class="math notranslate nohighlight">\(\alpha\)</span>, Sarsa learns the same final policy as Q-Learning, but more slowly. This experiment highlights the impact of parameter choices in reinforcement learning. <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\epsilon\)</span>, initial values, and the length of the experiment can all influence the final result.</p></li>
</ul>
</li>
<li><p><strong>Example of Cliff Walking - Another comparison between Sarsa and Q-learning</strong></p>
  <img src="../_static/img/chapter6/cliff_walking.png" alt="Video: Sarsa in the Windy Gridworld" style="width:95%;">
<ul class="simple">
<li><p><strong>Description</strong>:</p>
<ul>
<li><p>States and goal: the agent start at state <span class="math notranslate nohighlight">\(S\)</span> on the lower left and tries to reach the goal G on the lower right.</p></li>
<li><p>Actions: up, down, right, and left.</p></li>
<li><p>Reward: <span class="math notranslate nohighlight">\(-1\)</span> on all transitions except those into the region marked “The Cliff”, which incurs a reward of <span class="math notranslate nohighlight">\(-100\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Performance comparison</strong>:</p>
<ul>
<li><p>Q-learning (red): learns values for the optimal policy, that which travels right along the edge of the cliff. But this results in its occasionally falling off the cliff because of the <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection.</p></li>
<li><p>Sarsa (blue): learns the longer but safer path through the upper part of the grid.</p></li>
<li><p>Although Q-learning actually learns the values of the optimal policy, <strong>its online performance is worse than that of Sarsa, which learns the roundabout policy</strong>. Of course, if <span class="math notranslate nohighlight">\(\epsilon\)</span> were gradually reduced, then <strong>both methods would asymptotically converge to the optimal policy.</strong></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="expected-sarsa">
<h3>6.2.3 Expected Sarsa<a class="headerlink" href="#expected-sarsa" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Backgound for expected Sarsa</strong>:</p>
<ul class="simple">
<li><p>update rule:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
		\begin{align*}
		Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma E_{\pi}[Q(S_{t+1}, A_{t+1})|S_{t+1})] - Q(S_t, A_t)] \\
		\leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \sum_{a}\pi(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t)]
		\end{align*}
	\end{split}\]</div>
<ul class="simple">
<li><p>Given the next state, <span class="math notranslate nohighlight">\(S_{t+1}\)</span>, this update moves deterministically in the same direction as Sarsa moves in expectation, and accordingly it is called Expected Sarsa.</p></li>
<li><p>Expected Sarsa is more complex computationally than Sarsa but, in return, it eliminates the variance due to the random selection of <span class="math notranslate nohighlight">\(A_{t+1}\)</span>. Given the same amount of experience we might expect it to perform slightly better than Sarsa.</p></li>
</ul>
</li>
<li><p><strong>Expected Sarsa for estimating <span class="math notranslate nohighlight">\(\pi \approx \pi_{\star}\)</span></strong></p>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Turn expected Sarsa algo into book-similar image</p>
</div>
<ul class="simple">
<li><p>Algorithm parameter: step size <span class="math notranslate nohighlight">\(\alpha \in (0,1], \epsilon &gt; 0\)</span></p></li>
<li><p>Initialize <span class="math notranslate nohighlight">\(Q(s,a)\)</span>, for all <span class="math notranslate nohighlight">\(s \in S^+, a \in A(s)\)</span></p></li>
<li><p>Loop for each episode:</p>
<ul>
<li><p>Initialize <span class="math notranslate nohighlight">\(S\)</span></p></li>
<li><p>Loop for each step of episode:</p>
<ul>
<li><p>Choose <span class="math notranslate nohighlight">\(A\)</span> from <span class="math notranslate nohighlight">\(S\)</span> using policy derived from <span class="math notranslate nohighlight">\(Q\)</span> (e.g., <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy)</p></li>
<li><p>Take action <span class="math notranslate nohighlight">\(A\)</span>, observe <span class="math notranslate nohighlight">\(R, S'\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \sum_{a}\pi(a|S') Q(S', a) - Q(S, A)]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(S \leftarrow S'\)</span></p></li>
</ul>
</li>
<li><p>until <span class="math notranslate nohighlight">\(S\)</span> is terminal</p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The algorithm is just like Q-learning except that instead of using the maximum over next state-action pairs it uses the expected value, taking into account how likely each action is under the current policy.</p></li>
<li><p>The fun part about Expected Sarsa is that <strong>it can be both on- and off-policy</strong>. The above algorithm is a on-policy setting, but in general expected sarsa might use a policy different from the target policy <span class="math notranslate nohighlight">\(\pi\)</span> to generate behavior, in which case it becomes an off-policy algorithm.</p>
<ul>
<li><p>For example, suppose <span class="math notranslate nohighlight">\(\pi\)</span> is the greedy policy while behavior is more exploratory; then Expected Sarsa is then exactly Q-learning.</p></li>
<li><p>In the above sense Expected Sarsa subsumes and generalizes Q-learning while reliably improving over Sarsa. <strong>Except for the small additional computational cost, Expected Sarsa may completely dominate both of the other more-well-known TD control algorithms.</strong></p></li>
</ul>
</li>
</ul>
</div>
</li>
<li><p><strong>Comparison on Cliff Walking example</strong>:</p>
<p>The figure below shows the interim and asymptotic performance of the three TD control methods on the cliff-walking task as a function of <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<ul>
<li><p>All three algorithms use <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policy with <span class="math notranslate nohighlight">\(\epsilon\)</span>=0.1</p></li>
<li><p>Asymptotic performance is an average over 100,000 episodes, then averaged over 10 runs.</p></li>
<li><p>Interim performance is an average over the first 100 episodes, then averaged over 50,000 runs.</p>
  <img src="../_static/img/chapter6/comparison_cliff_walking.png" alt="Comparison of three TD control methods on cliff walking task" style="width: 60%;">
</li>
</ul>
<p>Expected Sarsa retains the significant advantage of Sarsa over Q-learning on this problem. In cliff walking the state transitions are all deterministic and all randomness comes from the policy. In such cases, Expected Sarsa can safely set <span class="math notranslate nohighlight">\(\alpha\)</span> = 1 without suffering any degradation of asymptotic performance, whereas Sarsa can only perform well in the long run at a small value of <span class="math notranslate nohighlight">\(\alpha\)</span>, at which short-term performance is poor.</p>
</li>
</ul>
</section>
</section>
<section id="summary">
<h2>6.3 Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>The methods presented in this chapter are today the most widely used reinforcement learning methods. This is probably due to their great simplicity: they can be applied online, with a minimal amount of computation, to experience generated from interaction with an environment; they can be expressed nearly completely by single equations that can be implemented with small computer programs.</p>
<p>The special cases of TD methods introduced in the present chapter should rightly be called <span class="math notranslate nohighlight">\(\textit{one-step, tabular, model-free}\)</span> TD methods. In the next chapter we extend them to the form that include a model of the environment. But for now, a quick summary:</p>
<ul>
<li><p><strong>Mindmap of where we are now</strong></p>
  <img src="../_static/img/chapter6/chapter6_mindmap.png" alt="Mindmap" style="width:100%;">
</li>
<li><p><strong>Key Takeaways</strong></p>
<ul>
<li><p><strong>TD Prediction (TD(0))</strong></p>
<ul>
<li><p>Updates value estimates after each step:</p>
<div class="math notranslate nohighlight">
\[V(S_t) ← V(S_t) + α (R_{t+1} + γ V(S_{t+1}) - V(S_t))\]</div>
</li>
<li><p>TD error (<span class="math notranslate nohighlight">\(δ_t\)</span>) measures the difference between predicted and updated values.</p></li>
<li><p>Advantages: No model needed, incremental updates, faster convergence than MC.</p></li>
</ul>
</li>
<li><p><strong>TD Control Methods</strong></p>
<ul class="simple">
<li><p>Sarsa (on-policy): Updates based on the current policy. Safer, but may not find the most optimal paths.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[Q(S_t, A_t) ← Q(S_t, A_t) + α [R_{t+1} + γ Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\]</div>
<ul class="simple">
<li><p>Q-learning (off-policy): Updates using the maximum possible future reward. Learns optimal policies but can be riskier during exploration.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[Q(S_t, A_t) ← Q(S_t, A_t) + α [R_{t+1} + γ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\]</div>
<ul class="simple">
<li><p>Expected Sarsa: Uses expected future rewards, reducing variance and improving stability.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[Q(S_t, A_t) ← Q(S_t, A_t) + α [R_{t+1} + γ Σ_a π(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t)]\]</div>
</li>
<li><p><strong>Comparisons</strong></p>
<ul class="simple">
<li><p>Sarsa learns conservative policies, good for risky environments.</p></li>
<li><p>Q-learning finds optimal policies but can perform poorly online due to risky exploration.</p></li>
<li><p>Expected Sarsa balances both, offering stable performance with minimal extra computation.</p></li>
</ul>
</li>
<li><p><strong>Convergence</strong>: All methods converge to optimal policies if exploration is sufficient, and learning rates are properly set.</p></li>
</ul>
</li>
<li><p><strong>Extra lecture video (optional)</strong>: <a class="reference external" href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/MgFyz/rich-sutton-the-importance-of-td-learning">Rich Sutton: The Importance of TD Learning</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "Dong237/DistilRLIntroduction",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Contents"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
<div class="section ablog__blog_comments">
   
</div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="5_monte_carlo_methods.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 5. Monte Carlo Methods</p>
      </div>
    </a>
    <a class="right-next"
       href="7_planning_learning_acting.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 7. Planning and Learning with Tabular Methods</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#td-prediction">6.1 TD prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#td-control">6.2 TD Control</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sarsa-on-policy-td-control">6.2.1 Sarsa: On-policy TD Control</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning-off-policy-td-control">6.2.2 Q-learning: Off-policy TD Control</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-sarsa">6.2.3 Expected Sarsa</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">6.3 Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Youxiang Dong
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Youxiang Dong.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>