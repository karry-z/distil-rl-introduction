# ğŸ“š è’¸é¦ç‰ˆå¼ºåŒ–å­¦ä¹ ï¼šç®€æ˜å¯¼è®º

[![è®¸å¯è¯](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![ç½‘ç«™](https://img.shields.io/badge/ç½‘ç«™-è®¿é—®ç«™ç‚¹-blue?logo=github)](https://datawhalechina.github.io/distil-rl-introduction/)
[![å…³äºä½œè€…](https://img.shields.io/badge/About%20the%20Author-GitHub-blue?logo=github)](https://github.com/Dong237)
[![GitHubæ˜Ÿæ ‡](https://img.shields.io/github/stars/Dong237/DistilRLIntroduction?style=social)](https://github.com/datawhalechina/distil-rl-introduction)

---

<span>[ <a href="README.md">English</a> | ä¸­æ–‡ ]</span>

## ğŸ“š é¡¹ç›®ç®€ä»‹

<img src="docs/\_static/img/logo.png" align="right" width="40%"/>

ä½œä¸ºä¸€åå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å­¦ä¹ è€…ï¼Œæˆ‘ä¸€ç›´åœ¨å¯»æ‰¾èƒ½å¤Ÿåœ¨éš¾åº¦å’Œå®ç”¨åº¦ä¹‹é—´è¾¾åˆ°é€‚å½“å¹³è¡¡çš„å­¦ä¹ èµ„æºã€‚Sutton çš„ã€Šå¼ºåŒ–å­¦ä¹ å¯¼è®ºã€‹è™½ç„¶æ˜¯å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„ç»å…¸ä¹‹ä½œï¼Œä½†ä»å¤´åˆ°å°¾é€šè¯»éœ€è¦ä»˜å‡ºå¤§é‡ç²¾åŠ›ã€‚

æ‰€ä»¥æˆ‘ç”¨è‡ªå·±çš„RLå­¦ä¹ ç¬”è®°åˆ¶ä½œäº†è¿™ä¸ªæ•™ç¨‹ï¼šä¸€ä¸ªç²¾ç®€çš„"çŸ¥è¯†åº“"ï¼Œå¸Œæœ›èƒ½å¸®åŠ©å…¶ä»–RLå…¥é—¨è€…æ›´å¿«æ›´è½»æ¾åœ°æŒæ¡æ ¸å¿ƒæ¦‚å¿µ

---

## ğŸ¯ é¡¹ç›®ç›®çš„ä¸ä½¿ç”¨æ–¹æ³•

> **ğŸ’¡ æ ¸å¿ƒç†å¿µï¼š** æœ¬æ•™ç¨‹ç²¾é€‰äº†ã€Šå¼ºåŒ–å­¦ä¹ å¯¼è®ºã€‹ä¸­çš„é‡è¦ç« èŠ‚ï¼Œå¹¶å°†å…¶ä¸[Courseraå¼ºåŒ–å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹](https://www.coursera.org/specializations/reinforcement-learning)çš„å†…å®¹èåˆï¼Œä»¥æä¾›æ›´é«˜æ•ˆçš„å­¦ä¹ ä½“éªŒã€‚

> **ğŸ› ï¸ ä½¿ç”¨æ–¹æ³•ï¼š** åœ¨[æœ¬ç½‘ç«™](https://datawhalechina.github.io/distil-rl-introduction/)é˜…è¯»å®Œæ•´æ•™ç¨‹ã€‚åœ¨ç¬¬0ç« ï¼ˆåºè¨€ï¼‰ä¸­ï¼Œä½ å°†æ‰¾åˆ°æ›´å¤šå…³äºè¿™ä¸ªé¡¹ç›®çš„æ¥é¾™å»è„‰ä»¥åŠæœ€ä½³ä½¿ç”¨æ–¹æ³•çš„è¯¦ç»†ä¿¡æ¯ã€‚

---

## ğŸ“‹ ç›®å½•

### ğŸŒŸ ä»‹ç»
- [ç¬¬0ç« ï¼šåºè¨€](docs/Contents/0_prelude.md)
- [ç¬¬1ç« ï¼šå¼ºåŒ–å­¦ä¹ ç®€ä»‹](docs/Contents/1_intro.md)

### ğŸ§® è¡¨æ ¼è§£å†³æ–¹æ³•
#### å¼ºåŒ–å­¦ä¹ åŸºç¡€
- [ç¬¬2ç« ï¼šå¤šè‡‚èµŒåšæœº](docs/Contents/2_multi_armed_bandits.md)
- [ç¬¬3ç« ï¼šé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹](docs/Contents/3_markov_decision_process.md)
- [ç¬¬4ç« ï¼šåŠ¨æ€è§„åˆ’](docs/Contents/4_dynamic_programming.md)

#### åŸºäºé‡‡æ ·çš„å­¦ä¹ æ–¹æ³•
- [ç¬¬5ç« ï¼šè’™ç‰¹å¡æ´›æ–¹æ³•](docs/Contents/5_monte_carlo_methods.md)
- [ç¬¬6ç« ï¼šæ—¶åºå·®åˆ†å­¦ä¹ ](docs/Contents/6_temporal_difference_learning.md)
- [ç¬¬7ç« ï¼šè§„åˆ’ã€å­¦ä¹ ä¸è¡ŒåŠ¨](docs/Contents/7_planning_learning_acting.md)

### ğŸ¤– è¿‘ä¼¼è§£å†³æ–¹æ³•
#### ä»·å€¼å‡½æ•°è¿‘ä¼¼
- [ç¬¬8ç« ï¼šåŸºäºè¿‘ä¼¼çš„on-policyé¢„æµ‹](docs/Contents/8_on_policy_prediction_with_approximation.md)
- [ç¬¬9ç« ï¼šåŸºäºè¿‘ä¼¼çš„on-policyæ§åˆ¶](docs/Contents/9_on_policy_control_with_approximation.md)

#### ç­–ç•¥è¿‘ä¼¼
- [ç¬¬10ç« ï¼šç­–ç•¥æ¢¯åº¦æ–¹æ³•](docs/Contents/10_policy_gradient_methods.md)
- [ç¬¬11ç« ï¼šç°ä»£ç­–ç•¥æ¢¯åº¦æ–¹æ³•](docs/Contents/11_modern_policy_gradient_methods.md)

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿å¤§å®¶ä¸ºæ”¹è¿›è¿™ä¸ªæ•™ç¨‹åšå‡ºè´¡çŒ®ï¼ä»¥ä¸‹æ˜¯å¯ä»¥æä¾›å¸®åŠ©çš„æ–¹å¼ï¼š

- **æŠ¥å‘Šé—®é¢˜**ï¼šå‘ç°æ‹¼å†™é”™è¯¯æˆ–è§£é‡Šä¸æ¸…æ¥šçš„åœ°æ–¹å¯ä»¥ç›´æ¥åœ¨githubæäº¤issueæˆ–PR
- **ä¼ æ’­åˆ†äº«**ï¼šä¸å¯èƒ½ä¼šè§‰å¾—è¿™ä¸ªæ•™ç¨‹æœ‰ç”¨çš„äººåˆ†äº«ï¼Œå¤šè°¢å„ä½daiä½¬
- **éœ€è¦ç¿»è¯‘**ï¼šåœ¨æœªæ¥æŸä¸ªæ—¶å€™æˆ‘å¯èƒ½(ä½†å¯èƒ½æ€§ä¸é«˜å˜¿å˜¿å˜¿)ç¿»è¯‘è¿™æœ¬ä¹¦çš„ä¸­æ–‡ç‰ˆæœ¬ï¼Œå¦‚æœæœ‰å…´è¶£å¯ä»¥å’Œæˆ‘ä¸€èµ·

---

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚é˜…[LICENSE](LICENSE)æ–‡ä»¶ã€‚
