
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 5. Monte Carlo Methods &#8212; DistilRLIntro 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=e1a75a79"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Contents/5_monte_carlo_methods';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 6. Temporal-Difference Learning" href="6_temporal_difference_learning.html" />
    <link rel="prev" title="Chapter 4. Dynamic Programming" href="4_dynamic_programming.html" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />   
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">⚠️This is an ongoing project and is currectly still under development (Chinese version on its way). ⚠️</div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="DistilRLIntro 0.1 documentation - Home"/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="DistilRLIntro 0.1 documentation - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">I. Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_prelude.html">Chapter 0. Prelude</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_intro.html">Chapter 1. Introduction to RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II.1. Fundamentals of Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_multi_armed_bandits.html">Chapte 2. Multi-armed Bandit</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_markov_decision_process.html">Chapter 3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_dynamic_programming.html">Chapter 4. Dynamic Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II.2. Sample-based Learning Methods</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 5. Monte Carlo Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_temporal_difference_learning.html">Chapter 6. Temporal-Difference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_planning_learning_acting.html">Chapter 7. Planning and Learning with Tabular Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III.1. Value Function Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="8_on_policy_prediction_with_approximation.html">Chapter 8. On-policy Prediction with Approximation</a></li>
<li class="toctree-l1"><a class="reference internal" href="9_on_policy_control_with_approximation.html">Chapter 9. On-policy Control with Approximation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III.2. Policy Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_policy_gradient_methods.html">Chapter 10. Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_modern_policy_gradient_methods.html">Chapter 11. Modern Policy Gradient Methods</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/blob/master/Contents/5_monte_carlo_methods.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/edit/master/Contents/5_monte_carlo_methods.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/issues/new?title=Issue%20on%20page%20%2FContents/5_monte_carlo_methods.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Contents/5_monte_carlo_methods.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 5. Monte Carlo Methods</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-prediction-evaluation">5.1 Monte Carlo Prediction (Evaluation)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mc-prediction-for-state-value-function">5.1.1 MC Prediction for state-value function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mc-prediction-for-action-value-function">5.1.2 MC Prediction for action-value function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control">5.2 Monte Carlo Control</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control-removing-assumption-2">5.2.1 Monte Carlo Control removing Assumption (2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control-removing-both-assumptions">5.2.2 Monte Carlo Control removing both assumptions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-methods">5.3 Off-policy Monte Carlo Methods</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-prediction-via-importance-sampling">5.3.1 Off-policy Monte Carlo Prediction via Importance Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-control">5.3.2 Off-policy Monte Carlo Control</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">5.4 Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                   <section class="tex2jax_ignore mathjax_ignore" id="chapter-5-monte-carlo-methods">
<h1>Chapter 5. Monte Carlo Methods<a class="headerlink" href="#chapter-5-monte-carlo-methods" title="Link to this heading">#</a></h1>
<p>Monte Carlo methods are ways of solving the reinforcement learning problem based on <strong>experience only</strong>, i.e., averaging sample returns. To ensure that well-defined returns are available, here we define Monte Carlo methods only for <strong>episodic tasks</strong>.</p>
<p>Only on the completion of an episode are value estimates and policies changed. Monte Carlo methods can thus be incremental in an <strong>episode-by-episode</strong> sense, but not in a step-by-step (online) sense.</p>
<p>We adapt the idea of general policy iteration (GPI) and learn value functions from sample returns with the MDP. The value functions and corresponding policies still interact to attain optimality in essentially the same way (GPI).</p>
<p>One final note about the notation: <strong>throughout this chapter, we assume an episode always starts at time step <span class="math notranslate nohighlight">\(0\)</span> and ends at time step <span class="math notranslate nohighlight">\(T\)</span>, i.e., <span class="math notranslate nohighlight">\(S_T\)</span> is the terminal state.</strong></p>
<section id="monte-carlo-prediction-evaluation">
<h2>5.1 Monte Carlo Prediction (Evaluation)<a class="headerlink" href="#monte-carlo-prediction-evaluation" title="Link to this heading">#</a></h2>
<p>Basics of MC methods:</p>
<ul class="simple">
<li><p><strong>Define a <span class="math notranslate nohighlight">\(\textit{visit}\)</span> to state <span class="math notranslate nohighlight">\(s\)</span></strong>: an occurrence of state s in an episode. Of course, <span class="math notranslate nohighlight">\(s\)</span> may be visited multiple times in the same episode</p></li>
<li><p><strong>Evaluation methods</strong>:</p>
<ul>
<li><p><strong>first-visit MC method</strong> estimates <span class="math notranslate nohighlight">\(v_{\pi}(s)\)</span> as the average of the returns following first visits to <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p><strong>every-visit MC method</strong> averages the returns following all visits to <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
</ul>
</li>
</ul>
<section id="mc-prediction-for-state-value-function">
<h3>5.1.1 MC Prediction for state-value function<a class="headerlink" href="#mc-prediction-for-state-value-function" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>First-visit MC prediction, for estimating <span class="math notranslate nohighlight">\(V \approx v_{\pi}\)</span></strong></p>
<ul>
<li><p>Algorithm:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter5/algo_first_visit_value.png" alt="Algorithm: First Visit Value" style="width: 100%;">
  </div>
</li>
<li><p>Intuition: the realizations of return <span class="math notranslate nohighlight">\(G_t\)</span> for each state is calculated backwards from <span class="math notranslate nohighlight">\(S_{T-1}\)</span> to <span class="math notranslate nohighlight">\(S_0\)</span>, by the law of large number, the average of <span class="math notranslate nohighlight">\(G_t\)</span> for each state will be the value of that state: <span class="math notranslate nohighlight">\(v(S_t) = E_{\pi}[G_t|S_t]\)</span></p></li>
<li><p>Visualization of return calculation (<span class="math notranslate nohighlight">\(T=5\)</span>, intergers on arrows are the rewards):</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter5/computing_gt.png" alt="Backward calculation of returns" style="width: 350px;">
  </div>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>An important fact about MC methods is that the estimates for each state are <strong>independent.</strong> The estimate for one state does not build upon the estimate of any other state, as is the case in DP. In other words, Monte Carlo methods do not bootstrap as we defined it in the previous chapter.</p></li>
<li><p>For MC methods, the computational expense of estimating the value of a single state is independent of the number of states. This can make Monte Carlo methods particularly attractive when one requires the value of only one or a subset of states.</p></li>
</ul>
</div>
</section>
<section id="mc-prediction-for-action-value-function">
<h3>5.1.2 MC Prediction for action-value function<a class="headerlink" href="#mc-prediction-for-action-value-function" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Motivation</strong>: state values <span class="math notranslate nohighlight">\(v_{\pi}\)</span> are only usable when we have the model of the environment. Since MC methods assume there is <strong>no model available</strong>, one of our primary goals in this case is to actually estimate <span class="math notranslate nohighlight">\(q_\star\)</span>.</p></li>
</ul>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Turn this algo below into a book-similar image</p>
</div>
<ul class="simple">
<li><p><strong>First-visit MC prediction, for estimating <span class="math notranslate nohighlight">\(Q \approx q_{\pi}\)</span></strong></p>
<ul>
<li><p>Algorithm:</p>
<ul>
<li><p>Input: a policy <span class="math notranslate nohighlight">\(\pi\)</span> to be evaluated</p></li>
<li><p>Initialize:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(Q(s, a) \in \mathbb{R}\)</span> arbitrarily, for all <span class="math notranslate nohighlight">\(s \in S, a \in A(s)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(Return(s, a) \leftarrow\)</span> an empty list for all <span class="math notranslate nohighlight">\(s \in S, a \in A(s)\)</span>.</p></li>
</ul>
</li>
<li><p>Loop forever (for each episode):</p>
<ul>
<li><p>Generate an episode following <span class="math notranslate nohighlight">\(\pi: S_0, A_0, R_1, S_1, A_1, ..., S_{T-1}, A_{T-1}, R_T, S_T\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(G_T \leftarrow 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \text{for } t \text{ in } \{T-1, T-2, ..., 0\}\)</span>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(G_{t} \leftarrow \gamma G_{t+1} + R_{t+1}\)</span></p></li>
<li><p>Append <span class="math notranslate nohighlight">\(G_{t}\)</span> to <span class="math notranslate nohighlight">\(Returns(S_t, A_t)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(q(S_t, A_t) \leftarrow average(Returns(S_t, A_t))\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Intuition: same as in <a class="reference internal" href="#mc-prediction-for-state-value-function">section 5.1.1</a>, remember that <span class="math notranslate nohighlight">\(q(s,a) = E_\pi[G_t | S_t, A_t]\)</span></p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Monte-Carlo return <span class="math notranslate nohighlight">\(G_t\)</span> provides an <strong>unbiased sample</strong> of the expected return at a given state, but due to stochasticity from the dynamics of the environment and policy, each reward <span class="math notranslate nohighlight">\(R_t\)</span> can be a random variable, the sum of which can result in a <strong>high variance estimator</strong> of the expected return.</p>
<p>Many modern RL methods (or at least the estimation of return) have been proposed aiming to alleviate this problem, several of them will be introduced in <a class="reference internal" href="11_modern_policy_gradient_methods.html"><span class="std std-doc">Chapter 11</span></a>.</p>
</div>
</section>
</section>
<section id="monte-carlo-control">
<h2>5.2 Monte Carlo Control<a class="headerlink" href="#monte-carlo-control" title="Link to this heading">#</a></h2>
<p>General problems and two basic assumptions we rely on in <a class="reference internal" href="#monte-carlo-prediction-evaluation">section 5.1</a>:</p>
<ul class="simple">
<li><p><strong>Problem of <span class="math notranslate nohighlight">\(\textit{maintaining exploration}\)</span></strong>: in estimating <span class="math notranslate nohighlight">\(q_{\pi}\)</span>, many state-action pairs may never be visited. E.g., if the policy is deterministic, many actions at a state may not be taken.</p>
<ul>
<li><p><strong>Assumption (1)</strong> of <span class="math notranslate nohighlight">\(\textit{exploring starts}\)</span>: episodes start in a state-action pair, and every pair has a nonzero probability of being selected as the start. (So every state-action pair will be visited an infinite number of times in the limit of an infinite number of episodes.)</p></li>
</ul>
</li>
<li><p><strong>Problem of estimating <span class="math notranslate nohighlight">\(\hat{q}_{\pi}(S_t, A_t)\)</span></strong>: by default, we used the law of large number and rely on the following assumption:</p>
<ul>
<li><p><strong>Assumption (2)</strong> of infinite number of episodes: policy evaluation can be done with infinite number of episodes (complete policy evaluation).</p></li>
</ul>
</li>
</ul>
<p>Apparently, these two assumptions are hardly truth in practice, so we are now going to introduce methods that remove them gradually.</p>
<section id="monte-carlo-control-removing-assumption-2">
<h3>5.2.1 Monte Carlo Control removing Assumption (2)<a class="headerlink" href="#monte-carlo-control-removing-assumption-2" title="Link to this heading">#</a></h3>
<ul>
<li><p>How to <strong>remove Assumption (2)</strong>:</p>
<ul class="simple">
<li><p>To avoid infinite number of episodes nominally required for policy evaluation, we could <strong>give up trying to complete policy evaluation</strong> before returning to policy improvement. Value iteration can be seen as an extrem example of this idea.</p></li>
<li><p>For Monte Carlo policy iteration it is natural to alternate between evaluation and improvement on an <strong>episode-by-episode</strong> basis. After each episode, the observed returns are used for policy evaluation, and then the policy is improved at all the states visited in the episode.</p></li>
</ul>
</li>
<li><p>Monte Carlo ES (Exploring Starts), for estimating <span class="math notranslate nohighlight">\(\pi \approx \pi_{\star}\)</span></p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter5/algo_mc_es.png" alt="Algorithm: Monte Carlo with Exploring Starts" style="width: 100%;">
  </div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The essential technique of above algorithm is that after each update of <span class="math notranslate nohighlight">\(Q(S_t,A_t)\)</span>, the improvement (greedification) will be made directly, which removes the second assumption.</p>
</div>
</li>
</ul>
</section>
<section id="monte-carlo-control-removing-both-assumptions">
<h3>5.2.2 Monte Carlo Control removing both assumptions<a class="headerlink" href="#monte-carlo-control-removing-both-assumptions" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>On &amp; Off-policy methods</strong>:</p>
<ul class="simple">
<li><p>On policy methods: attempt to evaluate or improve the policy that is used to make decisions. (e.g., MC with ES, dynamic programming etc.)</p></li>
<li><p>Off policy methods: evaluate or improve a policy different from that used to generate the data.</p></li>
</ul>
</li>
<li><p><strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-soft policies</strong>:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policy</strong>: as introduced in Chapter 2 <a class="reference internal" href="2_multi_armed_bandits.html#action-value-methods"><span class="std std-ref">section 2.2</span></a> all non-greedy action are given the minimal probability of selection <span class="math notranslate nohighlight">\(\frac{\epsilon}{|A(s)|}\)</span> (<strong>uniform distribution</strong>), the greedy action has the probability of <span class="math notranslate nohighlight">\(1 - \epsilon + \frac{\epsilon}{|A(s)|}\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policy is a type of <span class="math notranslate nohighlight">\(\epsilon\)</span>-soft policies. Among <span class="math notranslate nohighlight">\(\epsilon\)</span>-soft policies, <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policies are in some sense those that are closest to greedy.</p></li>
</ul>
</li>
<li><p><strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-soft policy</strong>: all actions have probability of <span class="math notranslate nohighlight">\(\pi(a|s)&gt;\frac{\epsilon}{|A(s)|}\)</span> for all states. This means that the agent explores all possible actions with non-zero probability <span class="math notranslate nohighlight">\(\frac{\epsilon}{|A(s)|}\)</span>, but <strong>not necessarily uniformly</strong>.</p></li>
</ul>
</li>
<li><p><strong>On-Policy first-visit Monte Carlo Control (with <span class="math notranslate nohighlight">\(\epsilon\)</span>-soft policy), for estimating <span class="math notranslate nohighlight">\(\pi \approx \pi_{\star}\)</span></strong></p>
<ul>
<li><p>Algorithm:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter5/algo_mc_control.png" alt="Algorithm: On-policy Monte Carlo control" style="width: 100%;">
  </div>
</li>
<li><p>Intuition: If we remove the assumption of exploring starts, we cannot simply improve the policy by making it greedy with respect to the current value function, because that would prevent further exploration of nongreedy actions. So on the basis of Monte Carlo ES (Exploring Starts) in <a class="reference internal" href="#monte-carlo-control-removing-assumption-2">section 5.2.1</a>, this algorithm removes the assumptions of exploring start by:</p>
<ul class="simple">
<li><p>defining the initial policy to be <span class="math notranslate nohighlight">\(\epsilon\)</span>-soft and,</p></li>
<li><p>updating the old policy to be <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policy during policy improvement.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The algorithm above also removes the second assumption (infinite episode) because the <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedification happens immediately after the calculation of the <span class="math notranslate nohighlight">\(Q(S_t, A_t)\)</span>.</p></li>
<li><p>Policy improvement theorem assures that any <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policy with respect to <span class="math notranslate nohighlight">\(q_\pi\)</span> is an improvement over any <span class="math notranslate nohighlight">\(\epsilon\)</span>-soft policy.</p></li>
<li><p>Note that we now only achieve the best policy among the <span class="math notranslate nohighlight">\(\epsilon\)</span>-soft policies, i.e, a near-optimal policy which still explores, not really the optimal policy.</p></li>
</ul>
</div>
</section>
</section>
<section id="off-policy-monte-carlo-methods">
<h2>5.3 Off-policy Monte Carlo Methods<a class="headerlink" href="#off-policy-monte-carlo-methods" title="Link to this heading">#</a></h2>
<p>All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? A more straightforward approach is to use two policies</p>
<p>Let’s recap On / Off-policy learning:</p>
<ul class="simple">
<li><p><strong>On-policy learning</strong>: learns the value or policy function for the current <span class="math notranslate nohighlight">\(\textit{target policy}\)</span> <span class="math notranslate nohighlight">\(\pi\)</span> that the agent is following. This means that the agent learns by interacting with the environment using the same policy that it is improving. <strong>On-policy methods are generally simpler and are considered first.</strong></p></li>
<li><p><strong>Off-policy learning</strong>: the learning is from data generated by <span class="math notranslate nohighlight">\(\textit{behavior policy}\)</span> <span class="math notranslate nohighlight">\(b\)</span> and is “off” the <span class="math notranslate nohighlight">\(\textit{target policy}\)</span> <span class="math notranslate nohighlight">\(\pi\)</span>. Off-policy methods require additional concepts and notation, and because the data is due to a different policy, <strong>off-policy methods are often of greater variance and are slower to converge.</strong> <br />
<strong>On the other hand, off-policy methods are more powerful and general.</strong> They include on-policy methods as the special case in which the target and behavior policies are the same.</p>
<ul>
<li><p>In order to use episodes from <span class="math notranslate nohighlight">\(b\)</span> to estimate values for <span class="math notranslate nohighlight">\(\pi\)</span>, we proceede based on the <strong>assumption of coverage</strong>: wherever <span class="math notranslate nohighlight">\(\pi(a|s) \ge 0\)</span>, <span class="math notranslate nohighlight">\(b(a|s) \ge 0\)</span> must also hold. This means, behavior policy <span class="math notranslate nohighlight">\(b\)</span> must be stohastic in states where it is not identical to the target policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p></li>
</ul>
</li>
</ul>
<section id="off-policy-monte-carlo-prediction-via-importance-sampling">
<h3>5.3.1 Off-policy Monte Carlo Prediction via Importance Sampling<a class="headerlink" href="#off-policy-monte-carlo-prediction-via-importance-sampling" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong><span class="math notranslate nohighlight">\(\textit{Importance Sampling}\)</span> Implementation</strong>: Here is a <a class="reference external" href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/XPxPd/importance-sampling">lecture video</a> to explain importance sampling should you find the textual derivation hard to understand.</p>
<ul>
<li><p>given existing samples from <span class="math notranslate nohighlight">\(X \sim b\)</span>, we want to estimate <span class="math notranslate nohighlight">\(E_{\pi}[X]\)</span> (of a different distribution)</p></li>
<li><p>derivation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        \begin{align*}
        E_{\pi}[X] &amp;= \sum_{x \in X} x \pi(x) \\
        &amp;= \sum_{x \in X} x \pi(x) \frac{b(x)}{b(x)} \\
        &amp;= \sum_{x \in X} x b(x) \frac{\pi(x)}{b(x)} \\
        &amp;= \sum_{x \in X} x b(x) \rho(x)  \quad (\rho(x) \text{ denotes } \frac{\pi(x)}{b(x)}) \\
        &amp;= E_{b}[X \rho(X)] \\
        &amp;\approx \frac{1}{n} \sum_{i=1}^n x_i \rho(x_i)
        \end{align*}
        \end{split}\]</div>
</li>
<li><p>the ratio <span class="math notranslate nohighlight">\(\rho(x)=\frac{\pi(x)}{b(x)}\)</span> is called the <strong>importance sampling ratio</strong>.</p></li>
</ul>
</li>
<li><p><strong>Importance Sampling for evaluating the target policy <span class="math notranslate nohighlight">\(\pi\)</span> in theory</strong>:</p>
<ol class="arabic">
<li><p>Calculating the importance sampling ratio <span class="math notranslate nohighlight">\(\frac{\pi}{b}\)</span> <strong>for one given state-action trajectory</strong></p>
<ul>
<li><p>Given a trajectory <span class="math notranslate nohighlight">\(S_{t+1}, A_{t+1}, S_{t+2}..., A_{T-1}, S_T\)</span>, starting from <span class="math notranslate nohighlight">\(S_t, A_t\)</span>, the probability of this trajectory is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
            \begin{align*}
            Pr (&amp;S_{t+1}, A_{t+1} ,..., S_{T-1}, A_{T-1}, S_T | S_t, A_t) \\
            &amp;= p(S_{t+1}|A_t, S_t)\pi(A_{t+1}|S_{t+1}) ... p(S_{T-1}|A_{T-2}, S_{T-2})\pi(A_{T-1}|S_{T-1})p(S_T|A_{T-1},S_{T-1}) \\
            &amp;= \Pi_{k=t+1}^{k={T-1}} p(S_k|A_{k-1}, S_{k-1})\pi(A_k|S_k) \times p(S_T|A_{T-1},S_{T-1})
            \end{align*}
            \end{split}\]</div>
<p>Therefore the <strong>importance sampling ratio</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
            \begin{align*}
            \rho_{t+1: T-1} &amp;= \frac{\Pi_{k=t+1}^{k={T-1}} p(S_k|A_{k-1}, S_{k-1})\pi(A_k|S_k) \times p(S_T|A_{T-1},S_{T-1})}{\Pi_{k=t+1}^{k={T-1}} p(S_k|A_{k-1}, S_{k-1})b(A_k|S_k) \times p(S_T|A_{T-1},S_{T-1})} \\
            &amp;= \frac{\Pi_{k={t+1}}^{k={T-1}} \pi(A_k|S_k)}{\Pi_{k={t+1}}^{k={T-1}} b(A_k|S_k)}
            \end{align*}
            \end{split}\]</div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The subscript of <span class="math notranslate nohighlight">\(\rho_{t: T-1}\)</span> corresponds to the sequence of actions in the trajectory, i.e., {<span class="math notranslate nohighlight">\(A_{t+1}, ..., A_{T-1}\)</span>}, then the trajectory stops at the terminal state <span class="math notranslate nohighlight">\(S_T\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\rho_{t+1: T-1}\)</span> only depends on the two policies, <strong>not the dynamics of the environment</strong>, which means importance sampling can be used in model-free RL problems.</p></li>
</ul>
</div>
</li>
<li><p>Estimating <span class="math notranslate nohighlight">\(q_{\pi}(s,a)\)</span> given <span class="math notranslate nohighlight">\(q_b(s,a) = E_b[G_t|S_t = s, A_t = a]\)</span> as:</p>
<div class="math notranslate nohighlight">
\[q_{\pi}(s,a)= E_b[\rho_{t+1:T-1} \times G_t|S_t = s, A_t=a]\]</div>
<ul class="simple">
<li><p>Intuition: note that the value function for <span class="math notranslate nohighlight">\(q\)</span>: <span class="math notranslate nohighlight">\(E[G_t|S_t = s, A_t=a]\)</span> is caculating an expection based on all given trajectories, so every <span class="math notranslate nohighlight">\(G_t\)</span> is a single realisation, which can be seen as the variable <span class="math notranslate nohighlight">\(x\)</span> in the derivation of importance sampling equation. <br />
The importance sampling ratio <span class="math notranslate nohighlight">\(\rho_{t+1:T-1}\)</span> is caculated on a trajectory basis to correspond to this trajectory-based charactor of <span class="math notranslate nohighlight">\(G_t\)</span> (and is used for multiplication with <span class="math notranslate nohighlight">\(G_t\)</span> directly).</p></li>
</ul>
</li>
</ol>
</li>
<li><p><strong>Importance sampling for evaluating the target policy <span class="math notranslate nohighlight">\(\pi\)</span> in practice</strong></p>
<ul>
<li><p><strong>Settings</strong>:</p>
<ul class="simple">
<li><p>About time steps: the time steps will be numbered in a way that increases across episode boundaries for convenience. That is, if the first episode of the batch ends in a terminal state at <span class="math notranslate nohighlight">\(t=100\)</span>, then the next episode begins at <span class="math notranslate nohighlight">\(t = 101\)</span>.</p></li>
<li><p>About notations:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(J(s,a)\)</span>: <strong>the set of time steps</strong> in which state action pair <span class="math notranslate nohighlight">\((s,a)\)</span> is visited (This is for an every-visit method; for a first-visit method, <span class="math notranslate nohighlight">\(J(s,a)\)</span> would only include time steps that were first visits to <span class="math notranslate nohighlight">\((s,a)\)</span> within their episodes).</p></li>
<li><p><span class="math notranslate nohighlight">\(T(t)\)</span>: <strong>the time step</strong> of the first terminal state from time step <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\{G_t\}_{t \in J(s,a)}\)</span>: the set of returns that pertain to state action pair <span class="math notranslate nohighlight">\((s,a)\)</span> from all episodes.</p></li>
<li><p><span class="math notranslate nohighlight">\(\{\rho_{t+1:T(t)-1}\}_{t \in J(s,a)}\)</span>: the importance sampling ratio for the trajectory <span class="math notranslate nohighlight">\(\{S_{t+1}, A_{t+1}, ..., S_{T(t)-1}, A_{T(t)-1}, S_{T(t)} \}\)</span>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Approaches</strong>:</p>
<ul>
<li><p>Ordinary importance sampling for evaluating target policy:</p>
<div class="math notranslate nohighlight">
\[
            Q(s,a) \dot= \frac{\sum_{t \in J(s)} \ \rho_{t+1:T(t)-1} \times G_t}{|J(s,a)|}
            \]</div>
</li>
<li><p>Weighted importance sampling for evaluating target policy:</p>
<div class="math notranslate nohighlight">
\[
            Q(s,a) \dot= \frac{\sum_{t \in J(s)} \ \rho_{t+1:T(t)-1} \times G_t}{\sum_{t \in J(s)} \ \rho_{t+1:T(t)-1}}
            \]</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li><p><strong>For first-visit methods</strong>:</p>
<p>Consider the estimates of their first-visit methods after observing a single return <span class="math notranslate nohighlight">\(G_t\)</span> from <span class="math notranslate nohighlight">\((s,a)\)</span>, in the weighted-average estimate, the estimate is equal to the observed return <span class="math notranslate nohighlight">\(G_t\)</span> independent of the ratio, its estimate in this case is <span class="math notranslate nohighlight">\(q_b(s,a)\)</span> rather than <span class="math notranslate nohighlight">\(q_\pi(s,a)\)</span>. In contrast, the estimate from ordinary method is always <span class="math notranslate nohighlight">\(q_\pi(s,a)\)</span>, but it could be highly volatile depending on the value of <span class="math notranslate nohighlight">\(\rho_{t+1:T(t)-1}\)</span>.</p>
<p>In summary, ordinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias converges asymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite.</p>
<p><strong>In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred.</strong> Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximation that we explore later.</p>
</li>
<li><p><strong>For every-visit methods</strong>:</p>
<p>The every-visit methods for ordinary and weighed importance sampling are both biased, though, again, the bias falls asymptotically to zero as the number of samples increases. <strong>In practice, every-visit methods are often preferred because they remove the need to keep track of which states have been visited and because they are much easier to extend to approximations.</strong></p>
</li>
</ul>
</div>
<ul>
<li><p><strong>Incremental Implementation for updating <span class="math notranslate nohighlight">\(Q(s,a)\)</span></strong>:</p>
<ul>
<li><p><strong>For on-policy methods</strong>:</p>
<p><span class="math notranslate nohighlight">\(Q(s,a)\)</span> is calculated by simply averaging the collected return realizations, so the incremental implementation can be done the same way as in Chapter 2 in <a class="reference internal" href="2_multi_armed_bandits.html#stationary-problems"><span class="std std-ref">section 2.4.1</span></a>, namely:</p>
<div class="math notranslate nohighlight">
\[
        NewEstimate \leftarrow OldEstimate + StepSize*[Target - OldEstimate]
        \]</div>
</li>
<li><p><strong>For off-policy methods</strong>:</p>
<ul>
<li><p>Ordinary importance sampling: the returns are also simply averaged by <span class="math notranslate nohighlight">\(J(s,a)\)</span>, so the incremental rule is the same as on-policy methods, as shown above.</p></li>
<li><p>Weighted importance sampling: here we have to form a weighted average of the returns using a slightly different incremental algorithm.</p>
<ul>
<li><p>Assume the set <span class="math notranslate nohighlight">\(\{G_t\}_{t \in J(s,a)}\)</span> alredy contains <span class="math notranslate nohighlight">\(n-1\)</span> items, numbered as <span class="math notranslate nohighlight">\(G_1, G_2, ..., G_{n-1}\)</span>, and the respective weight for <span class="math notranslate nohighlight">\(G_i\)</span> is <span class="math notranslate nohighlight">\(W_i = \rho_{i+1:T(i)-1}\)</span> and <span class="math notranslate nohighlight">\(W_i \in \{\rho_{t+1:T(t)-1}\}_{t \in J(s,a)}\)</span></p></li>
<li><p>So the <span class="math notranslate nohighlight">\(n\)</span>-th weighted average estimate for <span class="math notranslate nohighlight">\(Q(s,a)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
                \begin{align*}
                Q_n(s,a) &amp;\dot= \frac{\sum_{k=1}^{k=n-1}W_k G_k}{\sum_{k=1}^{k=n-1}W_k} \\
                &amp;\dot= Q_{n-1}(s,a) + \alpha \times [G_n - Q_{n-1}(s,a)]
                \end{align*}
                \end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(\alpha = \frac{W_n}{C_n}\)</span> and <span class="math notranslate nohighlight">\(C_n \dot= C_{n-1} + W_n = \sum_{i=1}^{i=n} W_i\)</span>, where <span class="math notranslate nohighlight">\(C_0 \dot= 0\)</span></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Off-policy MC prediction (policy evaluation) for estimating <span class="math notranslate nohighlight">\(Q \approx q_\pi\)</span></strong></p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter5/algo_off_policy_mc_prediction_action.png" alt="Algorithm: Off-policy Monte Carlo Prediction for Action Value" style="width: 100%;">
  </div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(W_{t+1} = \rho_{t+1:T(i)-1}\)</span></p>
</div>
</li>
</ul>
</section>
<section id="off-policy-monte-carlo-control">
<h3>5.3.2 Off-policy Monte Carlo Control<a class="headerlink" href="#off-policy-monte-carlo-control" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Rule of thumb</strong>: In Control, the target policy is typically the deterministic greedy policy with respect to the current estimate of the action-value function. This policy becomes as deterministic optimal policy while the behavior policy remains stochastic and more exploratory, for example, an <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policy.</p></li>
<li><p><strong>Off-policy MC prediction (policy evaluation) for estimating <span class="math notranslate nohighlight">\(\pi \approx \pi_\star\)</span></strong></p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter5/algo_off_policy_mc_control.png" alt="Algorithm: Off-policy Monte Carlo Control" style="width: 100%;">
  </div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The policy <span class="math notranslate nohighlight">\(\pi\)</span> converges to optimal at all encountered states even though actions are selected according to a different soft policy <span class="math notranslate nohighlight">\(b\)</span>,which may change between or even within episodes.</p></li>
<li><p>A potential problem is that this method learns only from the tails of episodes, when all of the remaining actions in the episode are greedy. If nongreedy actions are common, then learning will be slow, particularly for states appearing in the early portions of long episodes. Potentially, this could greatly slow learning.</p></li>
</ul>
</div>
</li>
</ul>
</section>
</section>
<section id="summary">
<h2>5.4 Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Currently, Monte Carlo methods for both prediction and control remain unsettled and are a subject of ongoing research.</p>
<ul>
<li><p><strong>Mindmap of where we are now</strong></p>
  <img src="../_static/img/chapter5/chapter5_mindmap.png" alt="Mindmap" style="width:100%;">
</li>
<li><p><strong>Key Takeaways</strong></p>
<ul class="simple">
<li><p>Advantages of MC over DP methods:</p>
<ol class="arabic simple">
<li><p>Monte Carlo methods require no model of the environment’s dynamics.</p></li>
<li><p>Monte Carlo methods can be used with simulation or sample models. For surprisingly many applications it is easy to simulate sample episodes even though it is diffcult to construct the kind of explicit model of transition probabilities.</p></li>
<li><p>It is easy and effcient to use Monte Carlo methods to focus on a small subset of the states. A region of special interest can be accurately evaluated without going to the expense of accurately evaluating the rest of the state set.</p></li>
<li><p>MC methods may be less harmed by violations of the Markov property. Because they do not update their value estimates on the basis of the value estimates of successor states, i.e., they do not bootstrap.</p></li>
</ol>
</li>
<li><p>Maintaining sufficient exploration:</p>
<ul>
<li><p>For on-policy methods: use assumption (1) of exploring start or initialize <span class="math notranslate nohighlight">\(\pi\)</span> to be a <span class="math notranslate nohighlight">\(\epsilon\)</span>-soft policy</p></li>
<li><p>For off-policy methods: we learn the value function of a target policy from data generated by a different behavior policy, which satisfies the assumption of coverage. At the same time, we need importance sampling to transform the expected returns from the behavior policy to the target policy.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Extra lecture video (optional)</strong>: <a class="reference external" href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/mZvQp/emma-brunskill-batch-reinforcement-learning">Emma Brunskill: Batch Reinforcement Learning</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "Dong237/DistilRLIntroduction",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Contents"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
<div class="section ablog__blog_comments">
   
</div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="4_dynamic_programming.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 4. Dynamic Programming</p>
      </div>
    </a>
    <a class="right-next"
       href="6_temporal_difference_learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 6. Temporal-Difference Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-prediction-evaluation">5.1 Monte Carlo Prediction (Evaluation)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mc-prediction-for-state-value-function">5.1.1 MC Prediction for state-value function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mc-prediction-for-action-value-function">5.1.2 MC Prediction for action-value function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control">5.2 Monte Carlo Control</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control-removing-assumption-2">5.2.1 Monte Carlo Control removing Assumption (2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control-removing-both-assumptions">5.2.2 Monte Carlo Control removing both assumptions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-methods">5.3 Off-policy Monte Carlo Methods</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-prediction-via-importance-sampling">5.3.1 Off-policy Monte Carlo Prediction via Importance Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-control">5.3.2 Off-policy Monte Carlo Control</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">5.4 Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Youxiang Dong
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Youxiang Dong.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>